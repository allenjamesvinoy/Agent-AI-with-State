{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_state = {\n",
    "    \"user_info\": {},\n",
    "    \"topics\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shared_state(interaction_result, state_key):\n",
    "    if interaction_result and isinstance(interaction_result, dict):\n",
    "        shared_state[state_key] = interaction_result\n",
    "\n",
    "def extract_last_message(agent, chat_id):\n",
    "    if hasattr(agent, 'chat_messages') and chat_id in agent.chat_messages:\n",
    "        messages = agent.chat_messages[chat_id]\n",
    "        if messages:\n",
    "            return messages[-1].get('content', '')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_interaction(chat_config):\n",
    "    sender = chat_config[\"sender\"]\n",
    "    recipient = chat_config[\"recipient\"]\n",
    "    message = chat_config[\"message\"]\n",
    "    max_turns = chat_config.get(\"max_turns\", 1)\n",
    "    summary_method = chat_config.get(\"summary_method\")\n",
    "    summary_args = chat_config.get(\"summary_args\", {})\n",
    "    clear_history = chat_config.get(\"clear_history\", False)\n",
    "    context = chat_config.get(\"context\", {})\n",
    "    \n",
    "    print(f\"\\n--- Starting interaction: {sender.__class__.__name__} -> {recipient.__class__.__name__} ---\")\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Context: {context}\")\n",
    "    \n",
    "    if hasattr(sender, 'initiate_chat'):\n",
    "        sender.initiate_chat(\n",
    "            recipient,\n",
    "            message=message,\n",
    "            clear_history=clear_history,\n",
    "            max_turns=max_turns,\n",
    "            context=context\n",
    "        )\n",
    "    else:\n",
    "        # Fallback if initiate_chat doesn't exist\n",
    "        print(f\"WARNING: initiate_chat method not found on {sender.__class__.__name__}\")\n",
    "        # Try a basic send operation if available\n",
    "        if hasattr(sender, 'send'):\n",
    "            sender.send(message, recipient)\n",
    "            for _ in range(max_turns - 1):\n",
    "                if hasattr(recipient, 'get_response'):\n",
    "                    response = recipient.get_response()\n",
    "                    sender.send(response, recipient)\n",
    "    \n",
    "    # Generate summary if method specified\n",
    "    summary = None\n",
    "    if summary_method == \"reflection_with_llm\":\n",
    "        # In a real implementation, you'd use an actual LLM for summary\n",
    "        if \"summary_prompt\" in summary_args:\n",
    "            prompt = summary_args[\"summary_prompt\"]\n",
    "            print(f\"Generating summary using prompt: {prompt}\")\n",
    "            \n",
    "            # In a real implementation, you would:\n",
    "            # 1. Extract relevant conversation content\n",
    "            # 2. Send it to an LLM with the prompt\n",
    "            # 3. Parse the result\n",
    "            \n",
    "            # For this example, we'll use mock data based on the message content\n",
    "            if \"name\" in prompt and \"location\" in prompt:\n",
    "                # Mock extraction for first chat\n",
    "                if \"John\" in message and \"New York\" in message:\n",
    "                    summary = {\"name\": \"John\", \"location\": \"New York\"}\n",
    "                else:\n",
    "                    summary = {\"name\": \"Unknown\", \"location\": \"Unknown\"}\n",
    "            elif \"topics\" in prompt:\n",
    "                # Mock extraction for topic preferences\n",
    "                if \"AI\" in message:\n",
    "                    summary = {\"topics\": [\"AI\"]}\n",
    "                else:\n",
    "                    summary = {\"topics\": [\"general\"]}\n",
    "    \n",
    "    # Call the callback if provided\n",
    "    if \"callback\" in chat_config and summary:\n",
    "        chat_config[\"callback\"](summary)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = [\n",
    "    {\n",
    "        \"sender\": customer_proxy_agent,\n",
    "        \"recipient\": onboarding_personal_information_agent,\n",
    "        \"message\": \"Hi, I'd like to get started with your product. My name is John and I'm from New York.\",\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"summary_args\": {\n",
    "            \"summary_prompt\": \"Return the customer information \"\n",
    "                             \"into as JSON object only: \"\n",
    "                             \"{'name': '', 'location': ''}\",\n",
    "        },\n",
    "        \"max_turns\": 2,\n",
    "        \"clear_history\": True,\n",
    "        \"callback\": lambda result: update_shared_state(result, \"user_info\")\n",
    "    },\n",
    "    {\n",
    "        \"sender\": customer_proxy_agent,\n",
    "        \"recipient\": onboarding_topic_preference_agent,\n",
    "        \"message\": \"I'm interested in reading about AI.\",\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"summary_args\": {\n",
    "            \"summary_prompt\": \"Return the customer topic preferences \"\n",
    "                             \"into as JSON object only: \"\n",
    "                             \"{'topics': []}\",\n",
    "        },\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": False,\n",
    "        \"callback\": lambda result: update_shared_state(result, \"topics\")\n",
    "    },\n",
    "]\n",
    "\n",
    "# Execute the first two chats to gather information\n",
    "for chat in chats:\n",
    "    result = run_chat_interaction(chat)\n",
    "    print(f\"Interaction result: {result}\")\n",
    "    print(f\"Current shared state: {shared_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engagement_chat():\n",
    "    topic_list = shared_state.get(\"topics\", {}).get(\"topics\", [])\n",
    "    topic_str = \", \".join(topic_list) if topic_list else \"various topics\"\n",
    "    \n",
    "    return {\n",
    "        \"sender\": customer_proxy_agent,\n",
    "        \"recipient\": customer_engagement_agent,\n",
    "        \"message\": f\"Let's find something fun to read about {topic_str}.\",\n",
    "        \"context\": {\n",
    "            \"user_preferences\": shared_state\n",
    "        },\n",
    "        \"max_turns\": 1,\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "    }\n",
    "\n",
    "final_chat = create_engagement_chat()\n",
    "chats.append(final_chat)\n",
    "result = run_chat_interaction(final_chat)\n",
    "print(f\"Final interaction result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
